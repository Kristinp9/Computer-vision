{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelo sequencial para reconocimiento de imágenes.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPqWwymjfc7n"
      },
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "cat ~/.kaggle/kaggle.json #credenciales de kaggle generadas con la API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agSdDdeCfyLP"
      },
      "source": [
        "p_base= !kaggle datasets download -d jerzydziewierz/bee-vs-wasp #bajamos de kaggle los datasets de imagenes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjVMvDeWhJQL"
      },
      "source": [
        "!unzip bee-vs-wasp.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61R7G0EuhXV4"
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o33t_hbxhemo"
      },
      "source": [
        "labels = pd.read_csv(\"kaggle_bee_vs_wasp/labels.csv\", dtype=str)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-cY8IfCfU9c"
      },
      "source": [
        "duplicados = labels[labels.duplicated()]\n",
        "print(duplicados) #Chequeamos no tener valores duplicados. No tenemos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lC1DGnVinc2"
      },
      "source": [
        "import os\n",
        "labels.path = labels[\"path\"].str.replace(\"\\\\\",\"/\")\n",
        "labels.describe()\n",
        "#reemplazamos la doble barra en el nombre de archivos del dataset de etiquetas porque genera problemas para que flow from dataframe detecte el path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvIxWwp5ByNF"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_9gckP8BaK7"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(style='white',context='notebook')\n",
        "sns.countplot(labels['label'],palette='viridis');\n",
        "\n",
        "plt.title('clases'.title(),fontsize=15);\n",
        "plt.ylabel('count'.title(), fontsize=8)\n",
        "plt.xlabel('seccion'.title(), fontsize=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqG4vmHcOEnf"
      },
      "source": [
        "labels['label'].value_counts(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pVj1oupOW44"
      },
      "source": [
        "#dropeamos other y nos quedamos con 3 clases\n",
        "labels.drop(labels.loc[labels['label']=='other'].index, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hYLrPhRNiP"
      },
      "source": [
        "labels['label'].value_counts(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-2h9aVa3Um"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=labels.drop(columns=['label','id'])\n",
        "y=labels[\"label\"]\n",
        "\n",
        "train, val = train_test_split(labels, test_size=.2, random_state=42)\n",
        "val, test = train_test_split(val, test_size=.5, random_state=42)\n",
        "print('train:', train.shape, '\\nval:', val.shape, '\\ntest',  test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxJuzNN34hVD"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
        "        # Train \n",
        "train_g = datagen.flow_from_dataframe(train,\n",
        "            directory=\"kaggle_bee_vs_wasp/\",\n",
        "            x_col=\"path\",\n",
        "            y_col=\"label\",\n",
        "            has_ext=False,\n",
        "            subset=\"training\",\n",
        "            batch_size=64,\n",
        "            classes=None,\n",
        "            seed=42,\n",
        "            shuffle=True,\n",
        "            class_mode=\"categorical\",\n",
        "            target_size=(150, 150))\n",
        "\n",
        "\n",
        "        # Validation generator\n",
        "val_g = datagen.flow_from_dataframe(val,\n",
        "            directory=\"kaggle_bee_vs_wasp/\",\n",
        "            x_col=\"path\",\n",
        "            y_col=\"label\",\n",
        "            has_ext=False,\n",
        "            subset=\"validation\",\n",
        "            batch_size=64,\n",
        "            seed=42,\n",
        "            classes=None,\n",
        "            shuffle=True,\n",
        "            class_mode=\"categorical\",\n",
        "            target_size=(150, 150))    \n",
        "\n",
        "        # Test\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "test_g = test_datagen.flow_from_dataframe(test,\n",
        "            directory=\"kaggle_bee_vs_wasp/\",\n",
        "            x_col=\"path\",\n",
        "            y_col='label',\n",
        "            has_ext=False,\n",
        "            target_size=(150, 150),\n",
        "            class_mode=\"categorical\",\n",
        "            classes=None,\n",
        "            batch_size=64,\n",
        "            color_mode=\"rgb\",\n",
        "            seed=42,\n",
        "            shuffle=False)\n",
        "print(len(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV5kqzOVjSib"
      },
      "source": [
        "print('Total de imágenes en train:', len((val_g)))                                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDdse-lU5qfM"
      },
      "source": [
        "print('Total de imágenes en test:', len((test_g)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vyVF5TYkT3o"
      },
      "source": [
        "#chequeamos valores que tenemos en train\n",
        "labels[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuzWbDoccIbZ"
      },
      "source": [
        "classes = list(train_g.class_indices.keys())\n",
        "classes\n",
        "w = 10\n",
        "h = 10\n",
        "fig = plt.figure(figsize=(10, 15))\n",
        "columns = 4\n",
        "rows = 8\n",
        "ax = []\n",
        "x,y = train_g.next()\n",
        "for i in range(columns*rows):\n",
        "    image = x[i]\n",
        "    label = classes[list(y[i]).index(1)]\n",
        "    ax.append( fig.add_subplot(rows, columns, i+1) )\n",
        "    ax[-1].set_title(label)\n",
        "    plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DyGydTN1Z0F"
      },
      "source": [
        "classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53u5qRzscWtY"
      },
      "source": [
        "#generador de imagenes nuevas\n",
        "directory_path=None\n",
        "batch_size=64\n",
        "generador_train = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = generador_train.flow_from_dataframe(\n",
        "            train,\n",
        "            directory=\"kaggle_bee_vs_wasp/\",\n",
        "            x_col=\"path\",\n",
        "            y_col=\"label\",\n",
        "            has_ext=False,\n",
        "            subset=\"training\",\n",
        "            batch_size=64,\n",
        "            seed=42,\n",
        "            clases=None,\n",
        "            shuffle=True,\n",
        "            class_mode=\"categorical\",\n",
        "            target_size=(150, 150),\n",
        "            color_mode=\"rgb\")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_dataframe(\n",
        "            val,\n",
        "            directory=\"kaggle_bee_vs_wasp/\",\n",
        "            x_col=\"path\",\n",
        "            y_col=\"label\",\n",
        "            has_ext=False,\n",
        "            batch_size=64,\n",
        "            seed=42,\n",
        "            classes=None,\n",
        "            shuffle=False,\n",
        "            class_mode=\"categorical\",\n",
        "            target_size=(150, 150),\n",
        "            color_mode=\"rgb\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko4gy7hFUnOn"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from google.colab import files\n",
        "from keras.models import load_model\n",
        "checkpoint_model3 = ModelCheckpoint('from_scratch/weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_accuracy', verbose=0,\n",
        "                             save_best_only=False, save_weights_only=False, mode='auto')\n",
        "\n",
        "#files.download('model3.h5')\n",
        "\n",
        "\n",
        "callbacks=[checkpoint_model3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKqfMt5_65C1"
      },
      "source": [
        "#Probamos una red con data augmentation + regularización y dropout en la última capa\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dropout_rate=0.3\n",
        "model3 = models.Sequential()\n",
        "model3.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model3.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(layers.Flatten())\n",
        "model3.add(layers.Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.001)))\n",
        "model3.add(Dropout(dropout_rate))\n",
        "model3.add(layers.Dense(3, activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf81Xjbd8T1S"
      },
      "source": [
        "model3.summary()\n",
        "model3.save('model3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9FyqdW88ZKg"
      },
      "source": [
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUdgm7gwdd3C"
      },
      "source": [
        "#Antes de fitear vamos a equilibrar las clases \n",
        "from sklearn import preprocessing\n",
        "lb_classes = preprocessing.LabelBinarizer()\n",
        "\n",
        "# Ajustamos a la variable labels\n",
        "lb_classes.fit(labels['label'])\n",
        "lb_classes.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX7Yl0pIekRd"
      },
      "source": [
        "# Encodeamos las labels\n",
        "labels_ohe = pd.DataFrame(lb_classes.transform(labels['label']), columns=lb_classes.classes_, index=labels['id'])\n",
        "labels_ohe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "offZp0EEeJOg"
      },
      "source": [
        "class_weight = {k:v for k,v in zip(range(len(lb_classes.classes_)),\n",
        "                                   (1/labels_ohe.iloc[train.index].sum()) * 100)}\n",
        "\n",
        "class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXNjFBYsfKti"
      },
      "source": [
        "# Visualizamos los pesos definidos\n",
        "((1/labels_ohe.iloc[train.index].sum()) * 100).plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csn_P4QK8lqu"
      },
      "source": [
        "history3 = model3.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_generator.n//train_generator.batch_size, #train / batch size. \n",
        "      epochs=85,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.n//validation_generator.batch_size,\n",
        "      class_weight=class_weight,\n",
        "      callbacks=callbacks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZKRFZwcwt7Q"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model3, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET5XcdY8A8fS"
      },
      "source": [
        "files.download('model3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqSmUqX3WL_2"
      },
      "source": [
        "from keras.models import load_model\n",
        "model3.save('model3.h5')\n",
        "#model3 = load_model('model3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5diklsW2IPnV"
      },
      "source": [
        "acc3=history3.history['accuracy']\n",
        "val_loss3=history3.history['val_loss']\n",
        "loss3=history3.history['loss']\n",
        "val_acc3=history3.history['val_accuracy']\n",
        "epochs3 = range(1, len(acc3) + 1)\n",
        "\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.title('Modelo de base')\n",
        "plt.plot(epochs3,loss3)\n",
        "plt.plot(epochs3,val_loss3)\n",
        "plt.xticks(ticks=epochs3)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Training loss','Validation Loss'])\n",
        "\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.plot(epochs3,acc3)\n",
        "plt.plot(epochs3,val_acc3)\n",
        "plt.xticks(ticks=epochs3)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training Accuracy','Validation Accuracy']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQQgY358OL62"
      },
      "source": [
        "# Predecimos\n",
        "val_pred = model3.predict(validation_generator)\n",
        "\n",
        "# Obtenemos la matriz de etiquetas reales de validación\n",
        "val_labels = lb_classes.transform(val['label'])\n",
        "\n",
        "# Evaluamos las shapes de los arrays resultantes\n",
        "val_pred.shape, val_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdqeX1QuOMIh"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Definimos listas vacías para ir volcando las métricas por clase\n",
        "accuracy_per_class = []\n",
        "precision_per_class = []\n",
        "recall_per_class = []\n",
        "f1_per_class = []\n",
        "\n",
        "# Definimos un listado de etiquetas de clase\n",
        "classes = list(lb_classes.classes_)\n",
        "\n",
        "for i in range(val_labels.shape[1]):\n",
        "    # Computamos las métricas de evaluación por clase y hacemos un append a la lista correspondiente\n",
        "    accuracy_per_class.append(accuracy_score(val_labels[:, i], (val_pred[:, i] > 0.5).astype(int)).round(2))\n",
        "    precision_per_class.append(precision_score(val_labels[:,  i], (val_pred[:, i] > 0.5).astype(int)).round(2))\n",
        "    recall_per_class.append(recall_score(val_labels[:,  i], (val_pred[:, i] > 0.5).astype(int)).round(2))\n",
        "    f1_per_class.append(f1_score(val_labels[:,  i], (val_pred[:, i] > 0.5).astype(int)).round(2))\n",
        "    \n",
        "    print(classes[i])\n",
        "    print('Accuracy:', accuracy_per_class[i], 'Precision:', precision_per_class[i],\n",
        "          'Recall:', recall_per_class[i], 'F1:', f1_per_class[i])\n",
        "    \n",
        "    # Visualizamos la matriz de confusión por clase\n",
        "    cm  = confusion_matrix(val_labels[:,  i], (val_pred[:, i] > 0.5).astype(int))\n",
        "    sns.heatmap(cm, annot=True, cmap='Purples', fmt='.0f', cbar=False, square=True,)\n",
        "    plt.ylabel('Etiqueta real')\n",
        "    plt.xlabel('Etiqueta predicha')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kmqd6ajxw02"
      },
      "source": [
        "model3.metrics_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFEbAHDpyvk7"
      },
      "source": [
        "#generar best_threshold\n",
        "threshold = np.arange(0.1, 0.9, 0.05)\n",
        "\n",
        "# Definimos listas vacíos donde iremos volcando los resultados\n",
        "acc_ = []\n",
        "accs = []\n",
        "best_threshold = np.zeros(val_pred.shape[1])\n",
        "\n",
        "# Iteramos sobre las distintas clases\n",
        "for i in range(len(best_threshold)):\n",
        "    # Indexamos las probabilidades predichas por clase\n",
        "    y_prob = np.array(val_pred[:, i])\n",
        "    \n",
        "    # Iteramos sobre los posibles umbrales\n",
        "    for j in threshold:\n",
        "        # Binarizamos las predicciones de acuerdo al umbral\n",
        "        y_pred = [1 if prob >= j else 0 for prob in y_prob]\n",
        "        y_pred = np.array(y_pred)\n",
        "        # Calculamos accuracy\n",
        "        acc = accuracy_score(val_labels[:, i], y_pred)\n",
        "        acc_.append(acc)\n",
        "    \n",
        "    # Volcamos los resultados en las listas generales\n",
        "    acc_  = np.array(acc_)\n",
        "    index = np.where(acc_==acc_.max())\n",
        "    accs.append(acc_.max())\n",
        "    best_threshold[i] = threshold[index[0][0]]\n",
        "    # Vaciamos la lista de para volver a iterar\n",
        "    acc_ = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyUqfnxdkW0o"
      },
      "source": [
        "accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQc5hwhKIiDT"
      },
      "source": [
        "best_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cchlgkb_yuVh"
      },
      "source": [
        "# Tomamos estos umbrales para generar las clasificaciones\n",
        "y_pred = np.array([[1 if val_pred[i, j] >= best_threshold[j] else 0 for j in range(val_labels.shape[1])] for i in range(len(val_labels))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfRelA_G1KCb"
      },
      "source": [
        "model3.evaluate(test_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WThEddfQzyGh"
      },
      "source": [
        "#vemos como generaliza el modelo\n",
        "# Predecimos\n",
        "test_pred = model3.predict(test_g)\n",
        "\n",
        "# Tomamos los umbrales óptimos obtenidos en validación para mapear probabilidades a etiquetas de clase\n",
        "y_pred = np.array([[1 if test_pred[i, j] >= best_threshold[j] else 0 for j in range(test_pred.shape[1])] for i in range(len(test_pred))])\n",
        "\n",
        "# Creamos un DF para facilitar luego la visualización de las predicciones\n",
        "y_pred = pd.DataFrame(y_pred, index=test.index)\n",
        "\n",
        "# Evaluamos las shapes de los arrays resultantes\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVDLG1Kw1tsB"
      },
      "source": [
        "import time\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "data_dir= \"/content/kaggle_bee_vs_wasp\"\n",
        "\n",
        "n = 50\n",
        "bichos = list()\n",
        "\n",
        "for i in range(n):\n",
        "    random_bichos = np.random.choice(test['path'])\n",
        "    while random_bichos in bichos:\n",
        "        random_bichos = np.random.choice(test['path'])\n",
        "    bichos.append(random_bichos)\n",
        "\n",
        "    # Imagen original\n",
        "    random_bichos_path = os.path.join(data_dir, random_bichos)\n",
        "    test_image = load_img(random_bichos_path, target_size=(224, 224))\n",
        "    test_image = img_to_array(test_image)\n",
        " \n",
        "    # Predicción\n",
        "    prediction = y_pred.loc[test.loc[test['path'] == random_bichos].index[0]]\n",
        "    predicted_labels = (prediction > 0.5).astype(int)\n",
        "\n",
        "    # Etiquetas\n",
        "    labels = lb_classes.classes_[predicted_labels > 0]\n",
        "\n",
        "    # Visualizamos la imagen y su predicción\n",
        "    title = \"{}\\n Etiquetas reales = {}\\nEtiquetas predichas = {}\" \\\n",
        "                .format(test.loc[test['path'] == random_bichos, 'path'].values[0],\n",
        "                        test.loc[test['path'] == random_bichos, 'label'].values[0],\n",
        "                        labels)\n",
        "\n",
        "    plt.imshow(plt.imread(os.path.join(data_dir, random_bichos)))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show();\n",
        "    \n",
        "    if i > 0 and i % 10 == 0:\n",
        "        time.sleep(20)\n",
        "        clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMurba_X0_j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}